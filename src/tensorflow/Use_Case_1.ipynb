{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Use_Case_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ECtBlDFLDMxw",
        "6QqpXpX2Qlhm",
        "NR8O2ZEwSGac",
        "nFZE0lzT4Ogx",
        "ZbbqBUOr5dhN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "5mDYeKCE_HeM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## USE CASE 1-IMSP 1st DATA Science school\n",
        "\n",
        "We recall the use case here\n",
        "\n",
        "You are a software developper and you want to provide your clients with an application that predicts the fees and related waiting delays for they medical appointments. Clients can feed delay to predict the fees . \n",
        "\n",
        "\n",
        "Predictions will be based on data collected from different practitioners in a pilot city during a 6 months time. \n",
        "\n",
        "\n",
        "Let's start By Setting the working environment"
      ]
    },
    {
      "metadata": {
        "id": "ECtBlDFLDMxw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GO AHEAD and mount your Google Drive folder from this code"
      ]
    },
    {
      "metadata": {
        "id": "l7pRaIZSCW3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SvGVu3F3DXyY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Change your working directory to access your data. "
      ]
    },
    {
      "metadata": {
        "id": "3nxRR4K2DLMM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/ColabDev\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3kN5jFlRD1hD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!pip install xlrd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P9WCzR0P_ukG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bYm983xtESOR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**/ !!!!!! \\ Note that the previous commands should be run only once for environment configuration  / !!!!!! \\\\.**"
      ]
    },
    {
      "metadata": {
        "id": "NMFcsce2E13H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use case implemetation can now start with defining the required packages in this use case."
      ]
    },
    {
      "metadata": {
        "id": "aii8jSMqMWQX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data visualisation and preprocessing\n",
        "\n",
        "\n",
        "We start with Data visualization and preprocessing before building out our model. You will find terms like pipeline or Data flow to define this process. It is crutial for the two following reasons:\n",
        "-\tYou need to know what type of data you are dealing with and which feature are important for the task you are trying to achieve. \n",
        "-\tYou need to prepare your data to be efficiently usable during the training phase. Not just load and train. Normalization, One hot encoding, String Classification, Data augmentation and Class indexing are among the usual preparation items.\n"
      ]
    },
    {
      "metadata": {
        "id": "vcqsPamm_A8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kAd5WGicE8J9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Let's load the data"
      ]
    },
    {
      "metadata": {
        "id": "MyONxGj4QJHr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Enter you code here to load the data you can use pd.read_excel? plus enter have the description of the function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6QqpXpX2Qlhm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## *solution*"
      ]
    },
    {
      "metadata": {
        "id": "cIV5nkpJFIxi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_excel(\"DATASETS/child_doctor_appointments.xlsx\", sheetname = \"Pédiatres\")\n",
        "raw_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVtgGVAVKDdx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_df.info()#Gives a flate description of the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DYreKOp2z2TH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Above we get the description of the data content which guide us in the way we will exploit them. Note at first place the \"Ville\" column which is the only non-numeric one. The next function will thus not be able to deal with it.\n",
        "\n",
        "Let's get a statistical description on the data. What is your analysis?"
      ]
    },
    {
      "metadata": {
        "id": "tAeSDZvDRKpj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data statistical summary\n",
        "Let's get a statistical description on the data. What is your analysis?"
      ]
    },
    {
      "metadata": {
        "id": "j_FUZnE_K6p4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G1-yMjqu0heq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We focus next on the column of interest. Our objective is to precdict the fee overhead to pay compared to standard price of 28 EUR. What can we decide looking at the outputs?"
      ]
    },
    {
      "metadata": {
        "id": "5_--A8tj1K_-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we observe information about the relation between the colums. Seaborn package wraput nice plotting function that we will use here. \n",
        "\n",
        "Is this explained what you observe?"
      ]
    },
    {
      "metadata": {
        "id": "L_gNlAcdx6tA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "sns.heatmap(raw_df.corr()[['Dépassement d\\'honoraires']], annot=True, vmin=-1, vmax=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IDIzORxZS6AU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Show next the correlation matrix among your input features (Seaborn function is used here too)"
      ]
    },
    {
      "metadata": {
        "id": "FRVh-7oPzC70",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(raw_df.corr(), annot=True, square=True, vmin=-1, vmax=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KW6mYfpNTey1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next let's look at a histogramm of the data showing scales and regularity. You can also make a plain matrixplot of the data to visualize relations between the input features."
      ]
    },
    {
      "metadata": {
        "id": "UtNHnF6QPfWP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_df.hist(figsize=(10, 8))\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "guZOkIJ--Y9O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Use the Panda value_counts() function to list unique labels and their ratio counts in the data set\n",
        "#Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IQuG_irC1b_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After getting all those insights about the data we are now ready to make the preparation phase."
      ]
    },
    {
      "metadata": {
        "id": "NR8O2ZEwSGac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## solution"
      ]
    },
    {
      "metadata": {
        "id": "jGUHDp3uMQCm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(raw_df['Dépassement d\\'honoraires'].value_counts())/raw_df['Dépassement d\\'honoraires'].count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XSJMgi84W1-G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data normalization and loading"
      ]
    },
    {
      "metadata": {
        "id": "Q9Qr6sXBQj_7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Scikit-learn is often used in the data preparation phase. \n",
        "\n",
        "In particular what we will use is StandardScaler a simple normalization method. It individually scales the features such that they have zero mean and unit variance, so they all belong to a standard Normal(0, 1) distribution. Note that this doesn’t change the ordering of the feature values, it just changes the scale. It’s a simple yet extremely important trick."
      ]
    },
    {
      "metadata": {
        "id": "chJ96eyKzhuo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = raw_df.copy()\n",
        "df = df.drop(['Tarif','Date appel', 'Date Rendez-vous'], axis=1)\n",
        "\n",
        "std_fee = df['Dépassement d\\'honoraires'].std(axis=0) \n",
        "mean_fee = df['Dépassement d\\'honoraires'].mean(axis=0) \n",
        "\n",
        "ss = StandardScaler()\n",
        "scale_features = ['Délai rendez-vous', 'Dépassement d\\'honoraires']\n",
        "df[scale_features] = ss.fit_transform(df[scale_features])\n",
        "\n",
        "#Next we will encode the string column on the cities using one hot encoding feature from SkitLearn library.\n",
        "df_cat = pd.get_dummies(df['Ville'])\n",
        "df = df.drop('Ville', axis=1)\n",
        "df_cat.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2QROtR7hZByf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at our input data"
      ]
    },
    {
      "metadata": {
        "id": "--zhN4SKZJnu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7VhDRBRkTogl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Spliting the data\n",
        "Scikit-learn has a convenient train_test_split function. We only need to specify the fraction of the test set, in our case 30%. But first we convert our data from pandas dataframe to numpy array using the values attribute of the dataframe."
      ]
    },
    {
      "metadata": {
        "id": "nGm5_mZpSR38",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df.drop('Dépassement d\\'honoraires', axis=1).values\n",
        "y = df['Dépassement d\\'honoraires'].values\n",
        "\n",
        "# Shuffle the training set\n",
        "order = np.argsort(np.random.random(y.shape))\n",
        "X = X[order]\n",
        "y = y[order]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W4cWK6jaTEou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the NN model\n",
        "Let's build our model. Here, we'll use a Sequential model with two densely connected hidden layers, and an output layer that returns a single, continuous value.\n",
        "\n",
        "We use keras.layers library to create a Dense  layer.\n",
        "Please change the second layer size to 64 units. "
      ]
    },
    {
      "metadata": {
        "id": "RH-TmnXMNeBp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  #How many layers and what sizes\n",
        "  layer1_size = 6#12#4#64\n",
        "  layer2_size = 6#4#64\n",
        "  #How fast do you want to learn?\n",
        "  learning_rate = 0.1\n",
        "  \n",
        "  model = keras.Sequential([\n",
        "    #First layer\n",
        "    keras.layers.Dense(layer1_size, activation=tf.nn.relu,\n",
        "                       input_shape=(X_train.shape[1],)),\n",
        "    #Second layer\n",
        "    keras.layers.Dense(layer2_size, activation=tf.nn.relu),\n",
        "    \n",
        "    #Third layer\n",
        "    keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
        "  \n",
        "  #Compile the model specifying loss function, choosen optimizer and evaluation metric\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mae'])#mape\n",
        "  return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQtvau5S1Mc5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ]
    },
    {
      "metadata": {
        "id": "eIJ7uFXyTbZS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model is trained for 500 epochs, and record the training and validation accuracy in the history object."
      ]
    },
    {
      "metadata": {
        "id": "jz-GYd2dTqKT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Display training progress by printing a single dot for each completed epoch\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    if epoch % 100 == 0: print('')\n",
        "    print('.', end='')\n",
        "\n",
        "EPOCHS = 500\n",
        "\n",
        "#Fit the model parameters to the data while minimizing the loss. \n",
        "#Store training stats\n",
        "history = model.fit(X_train, y_train, epochs=EPOCHS,\n",
        "                    validation_split=0.2, verbose=0,\n",
        "                    callbacks=[PrintDot()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3VCmyRxB5PR8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history.history.keys()\n",
        "mape = history.history['mean_absolute_error']\n",
        "mape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E7XZZuzTT7-K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualize the model's training progress using the stats stored in the history object. We want to use this data to determine how long to train before the model stops making progress."
      ]
    },
    {
      "metadata": {
        "id": "kiBtR3WIUBZ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Abs Error')\n",
        "  plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
        "           label='Train Loss')\n",
        "  plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
        "           label = 'Val loss')\n",
        "  plt.legend()\n",
        "  plt.ylim([0, 1])\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zxxq7rZKUM6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now observe the model performance on the test set"
      ]
    },
    {
      "metadata": {
        "id": "WikESOPcUnXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[loss, mae] = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: EUR{:7.2f}\".format(mae * std_fee))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PoVewr4jUwdw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now make some predictions"
      ]
    },
    {
      "metadata": {
        "id": "h8ykYYZVVASh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.xlabel('True Fees [EUR]')\n",
        "plt.ylabel('Predictions [EUR]')\n",
        "plt.axis('equal')\n",
        "plt.xlim(plt.xlim())\n",
        "plt.ylim(plt.ylim())\n",
        "_ = plt.plot([-100, 100], [-100, 100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7UJPRp6bVSJ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "error = test_predictions - y_test\n",
        "plt.hist(error, bins = 50)\n",
        "plt.xlabel(\"Prediction Error\")\n",
        "_ = plt.ylabel(\"Count\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Twp29kjXucmX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What are your conclusions? \n",
        "\n",
        "We move now to another dataset with more data. In this dataset you have records of healthcare for a countrywise population. \n",
        "\n",
        "In this data set avalaible on Kaggle there are records of insurance charges for a bunch of people in the US from different profiles as well."
      ]
    },
    {
      "metadata": {
        "id": "UjdvLMDJ3-Pn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Read the \"insurance.csv\" file from the DATASETS folder\n",
        "\n",
        "#Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bKsZTzo_4XEl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Transform the input data to input to your model. \n",
        "#Convert the categorical inputs\n",
        "\n",
        "#Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nFZE0lzT4Ogx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## solution"
      ]
    },
    {
      "metadata": {
        "id": "aQ2YLLouVjOe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv(\"DATASETS/insurance.csv\")\n",
        "raw_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "74ljrnehWCed",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Convert string to int by \"get_dummies\"\n",
        "\n",
        "sex_cat = pd.get_dummies(raw_df['sex'],drop_first=True,prefix='sex')\n",
        "smoker_cat = pd.get_dummies(raw_df['smoker'],drop_first=True,prefix='smoker')\n",
        "region_cat = pd.get_dummies(raw_df['region'],drop_first=True,prefix='region')\n",
        "\n",
        "dummies = pd.concat([sex_cat,smoker_cat,region_cat],axis=1)\n",
        "\n",
        "df = raw_df.copy()\n",
        "#merge to the original table\n",
        "\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "27l1sxgd42EK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare data, input  and train"
      ]
    },
    {
      "metadata": {
        "id": "z49Sex6d5N89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Split dataset\n",
        "#Drop columns containing string value\n",
        "#Replace them with \"get_dummies\"\n",
        "X = df.drop(columns=['charges','sex','smoker','region']).values\n",
        "y = np.array(df['charges'])\n",
        "\n",
        "# Shuffle the training set\n",
        "order = np.argsort(np.random.random(y.shape))\n",
        "X = X[order]\n",
        "y = y[order]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W3nI8Fx75PsS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Build your model and train it. You can look above as cheating sheet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZbbqBUOr5dhN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Solution"
      ]
    },
    {
      "metadata": {
        "id": "Djw70FUBWkBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation=tf.nn.relu,\n",
        "                       input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.train.RMSPropOptimizer(0.01)\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mae'])\n",
        "  return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# Display training progress by printing a single dot for each completed epoch\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    if epoch % 100 == 0: print('')\n",
        "    print('.', end='')\n",
        "\n",
        "EPOCHS = 500\n",
        "\n",
        "# Store training stats\n",
        "history = model.fit(X_train, y_train, epochs=EPOCHS,\n",
        "                    validation_split=0.2, verbose=0,\n",
        "                    callbacks=[PrintDot()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQ-jv-1L5glx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plotting the outputs"
      ]
    },
    {
      "metadata": {
        "id": "vV5jnSWhXgkz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Abs Error')\n",
        "  plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
        "           label='Train Loss')\n",
        "  plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
        "           label = 'Val loss')\n",
        "  plt.legend()\n",
        "  plt.ylim([0, 20000])\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "69AdVSQkXyfR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.xlabel('True Fees [EUR]')\n",
        "plt.ylabel('Predictions [EUR]')\n",
        "plt.axis('equal')\n",
        "plt.xlim(plt.xlim())\n",
        "plt.ylim(plt.ylim())\n",
        "_ = plt.plot([0, 60000], [0, 60000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SGFfsj4ihG3n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b9Ved8-U5_Y-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prediction examples"
      ]
    },
    {
      "metadata": {
        "id": "Cy2jxWyjKS6u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's imagine 4 different people and see what charges on health care will be for them.\n",
        "\n",
        "1. **Essombe**: 19 years old, BMI 27.9, has no children, smokes, from northwest region.\n",
        "\n",
        "2. **Ishola**: 60 years old. BMI 40, 4 children, doesn't smoke, from soutwest region.\n",
        "\n",
        "3. **Khady**: 40 years old, BMI 50, 2 children, doesn't smoke, from southeast region.\n",
        "\n",
        "4. **Marcos**: 30 years old. BMI 31.2, no children, doesn't smoke, from northeast region.\n"
      ]
    },
    {
      "metadata": {
        "id": "grhKJC2iKKB2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_pop = pd.DataFrame([{'age' : 19,'sex' : \"Male\",'bmi' : 27.9,'children' : 0,'smoker' : \"yes\",'region' : \"northwest\"}, \n",
        "                        {'age' : 60, 'sex' : \"Male\",'bmi' : 40, 'children' : 4,'smoker' : \"no\", 'region' : \"soutwest\"},\n",
        "                        {'age' : 40, 'sex' : \"Female\",'bmi' : 50, 'children' : 2,'smoker' : \"no\", 'region' : \"southeast\"},                       \n",
        "                        {'age' : 30, 'sex' : \"Male\",'bmi' : 31.2,'children' : 0,'smoker' : \"no\", 'region' : \"northeast\"}],index = [\"Bob\",\"John\",\"Lisa\",\"Terry\"])\n",
        "\n",
        "sex_cat = pd.get_dummies(test_pop['sex'],drop_first=True,prefix='sex')\n",
        "smoker_cat = pd.get_dummies(test_pop['smoker'],drop_first=True,prefix='smoker')\n",
        "region_cat = pd.get_dummies(test_pop['region'],drop_first=True,prefix='region')\n",
        "\n",
        "dummies = pd.concat([sex_cat,smoker_cat,region_cat],axis=1)\n",
        "\n",
        "test_pop = test_pop.drop(columns=['sex','smoker','region'])\n",
        "#merge to the original table\n",
        "\n",
        "test_pop = pd.concat([test_pop,dummies],axis=1)\n",
        "test_pop.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rh6mNgKviv4s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction = model.predict(np.array(test_pop)).flatten()\n",
        "print(\"Health care charges for Essombe : EUR {:7.2f}\".format(round(prediction[0], 2)))\n",
        "print(\"Health care charges for Ishola : EUR {:7.2f}\".format(round(prediction[1], 2)))\n",
        "print(\"Health care charges for Khady : EUR {:7.2f}\".format(round(prediction[2], 2)))\n",
        "print(\"Health care charges for Marcos : EUR {:7.2f}\".format(round(prediction[3], 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}